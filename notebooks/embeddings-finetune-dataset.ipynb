{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01402d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52213cd3",
   "metadata": {},
   "source": [
    "## Loading a long PDF document (e.g. book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c459c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document\n",
    "document_location = \"curriculum-doc.pdf\"\n",
    "loader = PyPDFLoader(document_location)\n",
    "pages = loader.load()\n",
    "\n",
    "# Combine the pages, and replace the tabs with spaces\n",
    "text = \"\"\n",
    "\n",
    "for page in pages:\n",
    "    text += page.page_content\n",
    "    \n",
    "text = text.replace('\\t', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b389cdc",
   "metadata": {},
   "source": [
    "We split the text in order to have manageable chunks that are small enough to be used as context for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d6da810",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                               chunk_overlap=500)\n",
    "\n",
    "docs = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27a5aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize chat model\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 openai_api_key=openai_api_key,\n",
    "                 max_tokens=3000,\n",
    "                 model='gpt-3.5-turbo-1106',\n",
    "                 request_timeout=120\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "115c3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehension_questions(text):\n",
    "    # Define the prompt\n",
    "    prompt = f\"Please provide 3 short comprehension questions about the following technical text: {text}\"\n",
    "    \n",
    "    # Use the model to generate questions\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Extract and format the questions\n",
    "    questions = response.content.split(\"\\n\")[:3]\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b15cd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the aim of the project \"Does well-calibrated uncertainty come for free with federated learning?\"',\n",
       " '2. Who is the supervisor of the project \"Predicting transmembrane protein topology from 3D structure\"?',\n",
       " '3. What is the focus of the project \"Improvements to particle identification in 4D-LPTV data\"?']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_comprehension_questions(docs[23].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db1970ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_pattern(input_string):\n",
    "    # Define the regular expression pattern\n",
    "    # The pattern looks for the number-period-space sequence at the start of the string\n",
    "    pattern = r'^\\d+\\.\\s'\n",
    "\n",
    "    # Use re.sub() to replace the pattern with an empty string\n",
    "    stripped_string = re.sub(pattern, '', input_string)\n",
    "\n",
    "    return stripped_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe83cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_variations(question):\n",
    "    # Define the prompt\n",
    "    prompt = f\"Please provide 4 variations of the following question: {question}\"\n",
    "    \n",
    "    # Use the model to generate questions\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Extract and format the questions\n",
    "    questions = response.content.split(\"\\n\")\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "026d2545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 13 processed in 9.69 secondss\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57123, Requested 3031. Please try again in 154ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 14 processed in 11.81 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 58193, Requested 3036. Please try again in 1.229s. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 16 processed in 14.54 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 58066, Requested 3397. Please try again in 1.463s. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 17 processed in 7.88 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57278, Requested 3030. Please try again in 308ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 19 processed in 10.24 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 58378, Requested 3028. Please try again in 1.406s. Visit https://platform.openai.com/account/rate-limits to learn more..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 56749, Requested 3396. Please try again in 145ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 21 processed in 14.71 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57815, Requested 3039. Please try again in 854ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 24 processed in 12.52 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57171, Requested 3034. Please try again in 205ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 26 processed in 11.31 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 58031, Requested 3053. Please try again in 1.084s. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 27 processed in 13.68 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 56752, Requested 3394. Please try again in 146ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 30 processed in 11.74 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57380, Requested 3037. Please try again in 417ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 32 processed in 10.03 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57304, Requested 3043. Please try again in 347ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 33 processed in 15.16 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 56995, Requested 3384. Please try again in 379ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 34 processed in 9.01 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57781, Requested 3396. Please try again in 1.177s. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 35 processed in 11.72 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57933, Requested 3394. Please try again in 1.327s. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 37 processed in 16.18 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 58347, Requested 3396. Please try again in 1.743s. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 39 processed in 15.71 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57240, Requested 3037. Please try again in 277ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 40 processed in 14.48 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57785, Requested 3397. Please try again in 1.182s. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 41 processed in 7.89 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57021, Requested 3040. Please try again in 60ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 43 processed in 11.38 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57027, Requested 3037. Please try again in 64ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 44 processed in 13.53 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 58044, Requested 3031. Please try again in 1.075s. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 45 processed in 11.42 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 58195, Requested 3037. Please try again in 1.232s. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 46 processed in 12.18 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57450, Requested 3040. Please try again in 489ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 48 processed in 11.67 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57504, Requested 3024. Please try again in 528ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 49 processed in 12.76 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57024, Requested 3035. Please try again in 58ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 51 processed in 9.87 secondss\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57413, Requested 3031. Please try again in 444ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 52 processed in 12.94 seconds\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-2IXj1st94AWlcAZhM1dpWKNI on tokens per min (TPM): Limit 60000, Used 57265, Requested 3034. Please try again in 299ms. Visit https://platform.openai.com/account/rate-limits to learn more..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number 53 processed in 12.97 seconds\r"
     ]
    }
   ],
   "source": [
    "# list of all questions generated\n",
    "all_questions = []\n",
    "\n",
    "for idx, chunk in enumerate(docs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # generate questions based on text input\n",
    "    questions = generate_comprehension_questions(chunk.page_content)\n",
    "    \n",
    "    for question in questions:\n",
    "        # process and add question\n",
    "        q = strip_pattern(question)\n",
    "        all_questions.append((q, idx))\n",
    "        \n",
    "        # generate variations and add to list\n",
    "        q_variations = generate_question_variations(q)\n",
    "        for q_var in q_variations:\n",
    "            all_questions.append((strip_pattern(q_var), idx))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    iteration_time = end_time - start_time\n",
    "    \n",
    "    # log progress\n",
    "    print(f\"chunk number {idx} processed in {iteration_time:.2f} seconds\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49db53aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "809"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af2fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chunks.csv\", \"w\") as file:\n",
    "    for doc,i in zip(docs, range(len(docs))):\n",
    "        file.write(f\"Chunk number {i}\\n\")\n",
    "        file.write(doc.page_content+\"\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f5fd9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chunks.csv\", \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"question\", \"chunk\"])\n",
    "    \n",
    "    # Write the data\n",
    "    for doc in docs:\n",
    "        chunk = doc.page_content\n",
    "        writer.writerow([\"<insert question here>\", chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61030c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV file for writing\n",
    "with open('question_answer.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"question\", \"chunk\"])\n",
    "    \n",
    "    # Write the data\n",
    "    for question, idx in all_questions:\n",
    "        writer.writerow([question, docs[idx].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1fed6204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/jacobottensten/Documents/DTU/5th_semester/deep-learn/project/question_answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0aa8f446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the format of the teaching for the Dee...</td>\n",
       "      <td>02456 Deep learning 2023 - course plan and inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the teaching format structured for the ...</td>\n",
       "      <td>02456 Deep learning 2023 - course plan and inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you explain the teaching format used in th...</td>\n",
       "      <td>02456 Deep learning 2023 - course plan and inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What teaching style is employed in the Deep Le...</td>\n",
       "      <td>02456 Deep learning 2023 - course plan and inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the instructional approach for the Dee...</td>\n",
       "      <td>02456 Deep learning 2023 - course plan and inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>Who is supervising the project mentioned in th...</td>\n",
       "      <td>mobile phones, FPGAs, Raspberry Pi, etc. These...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Who is overseeing the project mentioned in the...</td>\n",
       "      <td>mobile phones, FPGAs, Raspberry Pi, etc. These...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>Who is managing the project mentioned in the t...</td>\n",
       "      <td>mobile phones, FPGAs, Raspberry Pi, etc. These...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>Who is in charge of the project mentioned in t...</td>\n",
       "      <td>mobile phones, FPGAs, Raspberry Pi, etc. These...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>Who is leading the project mentioned in the text?</td>\n",
       "      <td>mobile phones, FPGAs, Raspberry Pi, etc. These...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>809 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    What is the format of the teaching for the Dee...   \n",
       "1    How is the teaching format structured for the ...   \n",
       "2    Can you explain the teaching format used in th...   \n",
       "3    What teaching style is employed in the Deep Le...   \n",
       "4    What is the instructional approach for the Dee...   \n",
       "..                                                 ...   \n",
       "804  Who is supervising the project mentioned in th...   \n",
       "805  Who is overseeing the project mentioned in the...   \n",
       "806  Who is managing the project mentioned in the t...   \n",
       "807  Who is in charge of the project mentioned in t...   \n",
       "808  Who is leading the project mentioned in the text?   \n",
       "\n",
       "                                                 chunk  \n",
       "0    02456 Deep learning 2023 - course plan and inf...  \n",
       "1    02456 Deep learning 2023 - course plan and inf...  \n",
       "2    02456 Deep learning 2023 - course plan and inf...  \n",
       "3    02456 Deep learning 2023 - course plan and inf...  \n",
       "4    02456 Deep learning 2023 - course plan and inf...  \n",
       "..                                                 ...  \n",
       "804  mobile phones, FPGAs, Raspberry Pi, etc. These...  \n",
       "805  mobile phones, FPGAs, Raspberry Pi, etc. These...  \n",
       "806  mobile phones, FPGAs, Raspberry Pi, etc. These...  \n",
       "807  mobile phones, FPGAs, Raspberry Pi, etc. These...  \n",
       "808  mobile phones, FPGAs, Raspberry Pi, etc. These...  \n",
       "\n",
       "[809 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6cbc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
