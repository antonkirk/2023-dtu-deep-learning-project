{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4JFRdliaJnM",
        "outputId": "cb35b204-f948-4dcd-9924-f72324404bbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m699.4/699.4 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade faiss-cpu tiktoken chromadb youtube-transcript-api langchain openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r4CWIb80gejh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import YoutubeLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7RC1EidaAW3i"
      },
      "outputs": [],
      "source": [
        "class YouTubeGPT:\n",
        "  def __init__(self) -> None:\n",
        "    OPENAI_API_KEY = \"\" # keep secret!\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "    self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0.3)\n",
        "    self.embeddings = OpenAIEmbeddings()\n",
        "\n",
        "  def generate_questions(self, video_url: str, n: int) -> list:\n",
        "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
        "    transcript = loader.load()\n",
        "    llm = self.llm\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables = [\"transcript\", \"number\"],\n",
        "        template = \"\"\"\n",
        "        Video: {transcript}\n",
        "        You are an expert question maker. Given the above video, it is your job to create a quiz of {number} text questions.\n",
        "        Make sure that questions are not repeated and check that all questions are conforming to the video as well.\n",
        "        You should only use factual information from the video to create the questions. Do not include the answers.\n",
        "\n",
        "    \"\"\"\n",
        "    )\n",
        "    runnable = prompt_template | llm | StrOutputParser()\n",
        "    questions = runnable.invoke({\"transcript\": transcript, \"number\": n})\n",
        "    questions_list = questions.split('\\n')\n",
        "    return questions, questions_list\n",
        "\n",
        "  def get_answers(self, video_url: str, questions) -> list:\n",
        "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
        "    transcript = loader.load()\n",
        "    llm = self.llm\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables = [\"transcript\", \"questions\"],\n",
        "        template = \"\"\"\n",
        "        Video: {transcript}\n",
        "        You are an deep learning expert. Given the above video, it is your job to answer the following text questions: {questions}.\n",
        "        Make sure that the answers are not repeated and that the answers are conforming to the video.\n",
        "        You should only use factual information from the video to generate these answers. Do not include the questions in your response.\n",
        "    \"\"\"\n",
        "    )\n",
        "    runnable = prompt_template | llm | StrOutputParser()\n",
        "    answers = runnable.invoke({\"transcript\": transcript, \"questions\": questions})\n",
        "    answers_list = answers.split('\\n')\n",
        "    return answers, answers_list\n",
        "\n",
        "  def evaluate_answers(self, video_url: str, questions, answers) -> str:\n",
        "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
        "    transcript = loader.load()\n",
        "    llm = self.llm\n",
        "    evaluation_results = []  # To store evaluation results for each question\n",
        "\n",
        "    for q,a in zip(questions, answers):\n",
        "        print(q)\n",
        "        user_answer = input()\n",
        "        prompt_template = PromptTemplate(\n",
        "        input_variables=[\"question\", \"answer\", \"student_answer\", \"video\"],\n",
        "        template=\n",
        "        \"\"\"\n",
        "        You are a helpful tutor that can give constructive critism aimed to help a student improve their knowledge within the field of Deep Learning.\n",
        "        The student has been tasked to answer the following question: {question}.\n",
        "        The correct answer is: {answer}.\n",
        "        The user answered the following: {student_answer}.\n",
        "        Please rate the correctness of the user answer. If you are in doubt, you can fact check the literature here: {video}\n",
        "\n",
        "        \"\"\"\n",
        "        )\n",
        "        chain = prompt_template | llm | StrOutputParser()\n",
        "        grade = chain.invoke({'question': q, 'answer': a, 'student_answer': user_answer, \"video\": transcript})\n",
        "        print(\"\\n\"+grade)\n",
        "        print()\n",
        "\n",
        "  def yt_vector_db(self, video_url: str) -> FAISS:\n",
        "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
        "    transcript = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap  = 100)\n",
        "    docs = text_splitter.split_documents(transcript)\n",
        "    db = FAISS.from_documents(docs, self.embeddings)\n",
        "    return db\n",
        "\n",
        "  def get_summary(self, video_url: str) -> str:\n",
        "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
        "    transcript = loader.load()\n",
        "    llm = self.llm\n",
        "    chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "    response = chain.run(transcript)\n",
        "    return response\n",
        "\n",
        "  def get_response(self, db, query, k=4) -> str:\n",
        "    # 4097 tokens for gpt-3.5-turbo\n",
        "    docs = db.similarity_search(query, k=k) # We can send 4 relevant docs based on the query from the user as k = 4\n",
        "    content = \" \".join([doc.page_content for doc in docs]) # Join 4 docs together\n",
        "    llm = self.llm\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"question\", \"docs\"],\n",
        "        template=\n",
        "        \"\"\"\n",
        "        You are a helpful tutor that can answer questions about a video based one the video's transcript.\n",
        "\n",
        "        You should answer the following question: {question}\n",
        "        By searching in the following video transcript: {docs}\n",
        "\n",
        "        You should only use factual information from the transcript to answer the question.\n",
        "\n",
        "        If you do not have enough information to answer the question, respond: \"Sorry, I don't know this\".\n",
        "\n",
        "        You answers should be detailed.\n",
        "\n",
        "        \"\"\"\n",
        "        )\n",
        "    runnable = prompt_template | llm | StrOutputParser()\n",
        "    response = runnable.invoke({\"question\": query, \"docs\": content})\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzq8RB3rV2ID",
        "outputId": "916d28a9-a9f2-4ade-b69a-1ee8df813311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Questions:\n",
            "1. What is the main focus of week three in the deep learning course?\n",
            "2. How does the recurrent architecture differ from the standard feed forward neural network?\n",
            "3. How is speech represented in the deep speech architecture discussed in the video?\n",
            "Answers:\n",
            "1. The main focus of week three in the deep learning course is on recurrent neural networks and their applications for sequences such as time sequences or biological sequences.\n",
            "\n",
            "2. The recurrent architecture differs from the standard feed forward neural network by having connections over time, allowing for a memory effect where information from previous time steps is used to contribute to the activation of the network at the current time step.\n",
            "\n",
            "3. Speech is represented in the deep speech architecture by using a spectrogram, which involves taking time windows of the speech and then taking the Fourier transform to create a representation of the energy frequency content for different frequency bands over time. Additionally, the architecture includes a convolutional neural network in the first layer to integrate filters that attend to specific time periods and provide input to the network at each time step.\n",
            "1. What is the main focus of week three in the deep learning course?\n",
            "Recurrent Neural Networks\n",
            "\n",
            "The user's answer is correct. The main focus of week three in the deep learning course is on recurrent neural networks and their applications for sequences such as time sequences or biological sequences.\n",
            "\n",
            "2. How does the recurrent architecture differ from the standard feed forward neural network?\n",
            "Connections over time\n",
            "\n",
            "The user's answer \"Connections over time\" is correct. Recurrent architecture differs from standard feed forward neural networks by having connections over time, allowing for the network to have memory and to process sequential data. Therefore, the user's answer is correct.\n",
            "\n",
            "3. How is speech represented in the deep speech architecture discussed in the video?\n",
            "Spectragram is the representation\n",
            "\n",
            "The user's answer is incorrect. The correct representation of speech in the deep speech architecture discussed in the video is through a spectrogram, which is then processed by a convolutional neural network in the first layer. The recurrent architecture is used for connections over time, allowing for a memory effect where information from previous time steps is used to contribute to the activation of the network at the current time step. Therefore, the user's answer does not accurately represent the speech representation in the deep speech architecture.\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    gpt = YouTubeGPT()\n",
        "    url = \"https://www.youtube.com/watch?v=TTKXgRV3twU\" # RNN part 1 video a week 5 on Transformers & RNNs\n",
        "    #question = \"What is said about the bidirectional recurrent neural networks?\"\n",
        "    #db = gpt.yt_vector_db(url)\n",
        "    #print(f\"Answer to question: {gpt.get_response(db, question)} \\n\")\n",
        "    #print(f\"Summary of lecture: {gpt.get_summary(url)}\")\n",
        "    questions, questions_list = gpt.generate_questions(url, 3)\n",
        "    print(\"Questions:\\n\"+str(questions))\n",
        "    answers, answers_list = gpt.get_answers(url, questions)\n",
        "    print(\"Answers:\\n\"+str(answers))\n",
        "    print(gpt.evaluate_answers(url, questions_list, answers_list))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
